{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Assignment 2\n",
    "Marc Schmid, student number 13349752\n",
    "\n",
    "Jorge Salamenco, student number ...\n",
    "\n",
    "No. 31005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a look at the data run the code below, as it is a huge dataset we preprocessed it and extracted the star rating of the place and mapped it to a user and restaurant.\n",
    "The dataset contains reviews with review ID, user id, business id, stars, date, text, useful, funny and cool.\n",
    "We gave the buisnesses and the users numerical values, which are represented by the rows and columns of our matrix. The star rating of the business is represented by the value in the specific column and rows of our matrix. For example if user 100 rates restaurant 300 with 5 stars, the matrix $M[100,300]$ has the value 5. \n",
    "The data is than stored in sparse matrixes, which just store the indices and the values which are non-zero or in our case available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('yelp_academic_dataset_review.json') as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not been to yet based on a latent factor model.\n",
    "The yelp dataset can be downloaded by https://www.kaggle.com/yelp-dataset/yelp-dataset\n",
    "For simplicity we reduced the dataset and stored it in a numpy file. It saves space and will be added to the code.\n",
    "\n",
    "### Data Preprocessing \n",
    "\n",
    "- due to huge dataset of yelp, just used the recommendation values otherwise we would have needed a cluster to compute it\n",
    "- kill small amount of reviews ( removed restaurants with less then 10 reviews)\n",
    "- substract the mean of the values ( so shift the user means, as everybody rates slightly different) \n",
    "- split data in training, validation and test to prevent overfitting (and test the algorithm properly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import time\n",
    "import copy\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Code\n",
    "\n",
    "1. Loading the ratings into a Sparse matrix -> we tried to use sparse matrices as much as possible to safe memory and runtime\n",
    "2. We preprocess the data which means:\n",
    "    - shifting the user mean\n",
    "    - deleting small amounts of ratings for all the restaurants\n",
    "    -splitting the datasets\n",
    "3. We implemented a latent factor model which is based on an alternating optimization algorithm. This is introduced right infront of the method\n",
    "4. We analyzed the training process of the model\n",
    "5. We implemented a latent factor model which is also based on a alternating optimization algorithm. But instead of fitting the latent factor with closed form solutions we tried a stochastic gradient decent approach ( the algorithm is explained right infront of the method again) \n",
    "6. We analysed the training process of the second model\n",
    "7. We implemented a Hyperparameter search to find the best parameters for the gradient descent model\n",
    "8. We compare the models based on the problem solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ratings from a self generated file (based on yelp) that is way smaller then the 3GB yelp dataset from kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = np.max(ratings[:,0]) + 1\n",
    "num_restaurants = np.max(ratings[:,1]) + 1\n",
    "\n",
    "M = sp.lil_matrix((num_users, num_restaurants))\n",
    "for row in ratings:\n",
    "    M[row[0],row[1]] = row[2]\n",
    "M = sp.csr_matrix(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the recommendation more stable we recursively remove restaurants with less then 10 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(matrix, min_entries):\n",
    "    '''\n",
    "    input:\n",
    "        matrix: shape [N,D] -> gets processed\n",
    "        min_entries: int -> min number of nonzero entries per row&columns\n",
    "    returns: \n",
    "        matrix: shape[N'<=N, D'<=D]\n",
    "    '''\n",
    "    \n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    start = time.time()\n",
    "\n",
    "    while True:\n",
    "        N ,D = matrix.shape[0], matrix.shape[1]\n",
    "        matrix = matrix[matrix.getnnz(axis=1) > min_entries][:, matrix.getnnz(axis=0) > min_entries]\n",
    "        N_new ,D_new = matrix.shape[0], matrix.shape[1]\n",
    "        converged = (N_new - N == 0 and D_new - D == 0)\n",
    "        if converged:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shift the user mean to make the data more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    '''\n",
    "    input: \n",
    "        matrix [N,D]\n",
    "    output: \n",
    "        matrix [N,D]->with shifted user means\n",
    "        user_means [N]->to recover states\n",
    "    \n",
    "    '''\n",
    "    mean = np.array(matrix.sum(axis=1))\n",
    "    div = np.array(matrix.getnnz(axis=1))\n",
    "    user_means = mean[:,0]/div\n",
    "    \n",
    "    #subtract mean from each row\n",
    "    nonzero = matrix.nonzero()\n",
    "    matrix = matrix.tolil()\n",
    "    for i,j in zip(nonzero[0], nonzero[1]):\n",
    "        matrix[i,j] -= user_means[i]\n",
    "    matrix = matrix.tocsr()\n",
    "    \n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data insto training validation and test set:\n",
    "everything is vectorized, so that the c-api of python can apply the better running time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    '''\n",
    "    input: \n",
    "        matrix [N,D] \n",
    "        \n",
    "        n_validation int\n",
    "        \n",
    "        n_test int\n",
    "    \n",
    "    return: \n",
    "        \n",
    "        matrix_split [N,D]->test&val entries are 0 \n",
    "        \n",
    "        val_idx     -> indices of validation\n",
    "        \n",
    "        test_idx    -> indices of test\n",
    "        \n",
    "        val_values  -> values of validation indices\n",
    "        \n",
    "        test_values -> values of test indices\n",
    "    \n",
    "    '''\n",
    "    #random seed is used to reproduse the results\n",
    "    np.random.seed(4)\n",
    "    \n",
    "    # select validation data\n",
    "    rand_val = np.random.choice(matrix.getnnz(), n_validation, replace=False)\n",
    "    val_idx = (matrix.nonzero()[0][rand_val], matrix.nonzero()[1][rand_val])\n",
    "    val_values = np.array(matrix[val_idx]).reshape(n_validation,)\n",
    "    matrix_split = matrix.tolil()\n",
    "    for i,j in zip(val_idx[0], val_idx[1]):\n",
    "        matrix_split[i,j] = 0\n",
    "    matrix_split = matrix_split.tocsr()\n",
    "    \n",
    "    # select test data\n",
    "    rand_test = np.random.choice(matrix_split.getnnz(), n_test, replace=False)\n",
    "    test_idx = (matrix_split.nonzero()[0][rand_test], matrix_split.nonzero()[1][rand_test])\n",
    "    test_values = np.array(matrix_split[test_idx]).reshape(n_validation,)\n",
    "    matrix_split = matrix_split.tolil()\n",
    "    for i,j in zip(test_idx[0], test_idx[1]):\n",
    "        matrix_split[i,j] = 0\n",
    "    matrix_split = matrix_split.tocsr()\n",
    "    \n",
    "    \n",
    "    matrix_split.eliminate_zeros()\n",
    "    return matrix_split, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = preprocessing(M, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store away the nonzero indices of M before subtracting the row means.\n",
    "nonzero_indices = np.vstack((M_train.nonzero()[0], M_train.nonzero()[1])).T\n",
    "\n",
    "# Remove user means.\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[val_idx[0]]\n",
    "test_values_shifted = test_values - user_means[test_idx[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First algorithm: Alternating optimization with or without SVDs\n",
    "The alternating optimization algorithm was acutally a substep in the winning code of the Netflix challenge recommender system.\n",
    "The problem is that you have large sets of sparse data which is really challenging: similarity search is expensive because of high complexity of distance functions, highly correlated dimensions could cause trouble for some algorithms, we got the curse of dimensionality and its pretty hard to visiualize high dimensional data. With an SVD we can reduce the data to the correlating values ( data with high variance (big eigenvalues) and small variance ( low eigenvalues). The problem is that standard SVD is not working with sparse data, as for the rating system not many movies are rated.\n",
    "\n",
    "SVDs are basically optimization algorithms which are based on power iteration or  subspace iteration methods or minimize the sum of reconstruction errors for a matrix. $A = U \\Sigma V^T$ where A is the recommender matrix, $\\Sigma $ are the eigenvalues and U and V are matrixes of eigenvectors. So if we use SVDs on unseen data we get The ratings via $R = Q*P^T$ and $Q= U \\Sigma$ and $P=V$. So if we do the SVDs with minimum reconstruction error, we force different complications like that the sum of the reconstruction error is over all entries (no-rating = zero-rating) and we have missing entries and R (where classical SVDs is not defined). So our goal is to find $Q \\in \\mathcal{R}^{n\\times k }$ and $P \\in \\mathcal{R}^{d\\times k }$ \n",
    "such that \n",
    "$$ \\min_{P,Q} \\sum_{i,x\\in R} (r_{x,i}-q_xp_i^T)^2$$\n",
    "Therefoe we use Alternating Optimization as an approach (Alternating optimization is a Standard method for latent factor models. I.e.:it also gets used to solve Gaussian mixture models, where we first update the distributions and then the latent variables):\n",
    "\n",
    "The steps are : \n",
    "1. initialize $P,Q$ and $t=0$\n",
    "2. $P^{t+1} = argmin_P f(P,Q^t)$\n",
    "3. $Q^{t+1} = argmin_P f(P^{t+1},Q)$\n",
    "4. $t=t+1$\n",
    "5. goto 2 until values converge\n",
    "\n",
    "Since P and Q are fixed respectively, the problem reduces to an ordinary least sqaure regression problem which has a closed form solution.\n",
    "Drawbacks of alternating optimization is that the solution is only an approximation , there is no guarantee that the solution is close to the optimal solution and its highly dependant on initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    '''\n",
    "    input: \n",
    "        matrix [N,D]\n",
    "        k   int-> number of latent dim\n",
    "        init state of initalization (random or by svds)\n",
    "    \n",
    "    returns: \n",
    "        Q [N,k] initial Q matrix of a latent factor model\n",
    "        P [k,D] initial P matrix of a latent factor model\n",
    "    \n",
    "    '''\n",
    "    N, D = matrix.shape[0], matrix.shape[1]\n",
    "    if init == 'svd':\n",
    "        \n",
    "        Q, _, P = svds(matrix, k=k) \n",
    "    elif init == 'random':\n",
    "        \n",
    "        np.random.seed(6)\n",
    "        Q = np.random.uniform(size=(N,k))\n",
    "        P = np.random.uniform(size=(k,D))\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to split training and validation loss function, as then we have a better running time, as well as less usage of memory.\n",
    "$$\\mathcal{L} = \\sum (M-Q*P)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(P,Q,M,non_zero_idx):\n",
    "    '''\n",
    "    the loss function is the root sqaure error RSE\n",
    "    '''\n",
    "    return np.sum(np.array(M[non_zero_idx[:,0], non_zero_idx[:,1]]-(Q.dot(P))[non_zero_idx[:,0], non_zero_idx[:,1]])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_validation(val_values, val_idx, P, Q):\n",
    "    '''\n",
    "    the loss function is the root sqaure error RSE\n",
    "    '''\n",
    "    return np.sum(np.array(val_values-(Q.dot(P))[val_idx[0], val_idx[1]])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm :\n",
    "for P as well as for Q we have closed form solutions.\n",
    "Therefore the ordinary least sqaured solution for each iteration becomes\n",
    "the P-update is:\n",
    "$$ p_i^T = \\left(\\frac{1}{|R_{*,i}|}\\sum_{x \\in R_{*,i}} q_x^Tq_x\\right)^{-1} \\frac{1}{|R_{*,i}|}\\sum_{x \\in R_{*,i}} q_x^T r_{xi} $$\n",
    "The Q-update is:\n",
    "$$ q_x^T = \\left(\\frac{1}{|R_{i,*}|}\\sum_{x \\in R_{i,*}} p_x^Tp_x \\right)^{-1} \\frac{1}{|R_{i,*}|}\\sum_{x \\in R_{i,*}} p_x^T r_{ix} $$\n",
    "For computational reasons, we used the scipy least sqaured model, to fit the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=10, eval_every=1):\n",
    "    '''\n",
    "    input\n",
    "        M [N,D] -> matrix to factorize\n",
    "        \n",
    "        non_zero_idx -> indices of non zero entries\n",
    "        \n",
    "        k -> latent dimensions\n",
    "        \n",
    "        Val_idx -> validation indices\n",
    "        \n",
    "        val_values -> validation values\n",
    "        \n",
    "        reg_lambda -> regularization strength\n",
    "        \n",
    "    returns\n",
    "        best_Q -> best Q based on val loss\n",
    "        \n",
    "        best_P -> best P based on val loss\n",
    "        \n",
    "        validation_losses -> to plot loss over time\n",
    "        \n",
    "        train_losses -> to plot loss over time\n",
    "        \n",
    "        converged_after -> convergence point\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Q,P = initialize_Q_P(M, k, init=init)  \n",
    "  \n",
    "    train_losses, validation_losses, time_per_iteration = [], [], []\n",
    "    best_Q, best_P = Q, P\n",
    "    best_val = loss_validation(val_values, val_idx, P, Q)\n",
    "    if reg_lambda > 0:  \n",
    "        model = Ridge(alpha=reg_lambda)#, solver='lsqr')\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "    \n",
    "    # Update P and Q\n",
    "    for iters in range(max_steps):\n",
    "        start = time.time()\n",
    "        # Update P\n",
    "        for i in range(M.shape[1]):\n",
    "            indices = non_zero_idx[non_zero_idx[:,1] == i][:,0]\n",
    "            if indices.size > 0:\n",
    "                y_train = M[indices, i].A.flatten()\n",
    "                X_train = Q[indices]\n",
    "                model.fit(X_train, y_train)\n",
    "                P[:,i] = model.coef_\n",
    "        # Update Q\n",
    "        for x in range(M.shape[0]):\n",
    "            indices = non_zero_idx[non_zero_idx[:,0] == x][:,1]\n",
    "            if indices.size > 0:\n",
    "                y_train = M[x, indices].A.flatten()\n",
    "                X_train = P[:,indices].T\n",
    "                model.fit(X_train, y_train)\n",
    "                Q[x] = model.coef_\n",
    "        #append loss for plotting\n",
    "        train_losses.append(loss_function(P, Q, M, non_zero_idx))\n",
    "        validation_losses.append(loss_validation(val_values, val_idx, P, Q))\n",
    "        if iters > 1 and validation_losses[iters] < best_val and iters % eval_every == 0: \n",
    "            best_Q, best_P, best_val = Q, P, validation_losses[iters]\n",
    "            converged_after = iters * eval_every\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        end = time.time()\n",
    "        time_per_iteration.append(end - start)\n",
    "        print(\"Iteration %d,  training loss: %.3f, validation loss: %.3f\"\n",
    "              % (iters, train_losses[iters], validation_losses[iters]))\n",
    "        if patience == 0: break\n",
    "        if iters == max_steps - 1: converged_after = -1\n",
    "        \n",
    "    average_time = np.mean(np.array(time_per_iteration))\n",
    "    if converged_after == -1:\n",
    "        print(\"Did not converge in %d iterations, on average %.2f seconds per iteration.\" % (max_steps, average_time))\n",
    "    else:\n",
    "        print(\"Converged after %d iterations, on average %.2f seconds per iteration.\" % (converged_after, average_time))\n",
    "    \n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the hyperparameter search is an exhaustive search and can take a lot of time, we tested the algorithm with one combination of values here.\n",
    "**maybe we shoul run more than one optimization parameter if we get some more time**\n",
    "\n",
    "**jorge** -> maybe when you compare to the test sets you can run some simulations with different hyperparameter\n",
    "\n",
    "We run the alternating optimization algorithm with $k=100$ and $\\lambda=1$. We plot the training and validation losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_a, P_a, val_l_a, tr_l_a, conv_a = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=100, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.arange(0, len(val_l_a), 1)\n",
    "plt.plot(x, val_l_a)\n",
    "plt.title(s='Alternating optimization validation loss, k=100', loc='center')\n",
    "plt.xlabel(s='Training iteration')\n",
    "plt.ylabel(s='Validation loss')\n",
    "plt.xticks(range(0,len(x), 2))\n",
    "\n",
    "# second plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, tr_l_a)\n",
    "plt.title(s='Alternating optimization training loss, k=100', loc='center')\n",
    "plt.xlabel(s='Training iteration')\n",
    "plt.ylabel(s='Training loss')\n",
    "plt.xticks(range(0,len(x), 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above we see the alternating optimization validation and training loss.\n",
    "We see that the training loss is constantly shrinking, while the validation loss is rising after 3 iterations. That indicates that the model starts to overfit the data after the 3rd iteration. With too many degrees of freedom, the model starts fitting noise and the model fits too well the training data but does not generalize well to unseen test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Latent factorization using gradient descent\n",
    "\n",
    "We now use gradient descent to factorize our ratings matrix. We will try both (mini-) batch and stochastic gradient descent.\n",
    "\n",
    "Recall that the objective function (loss) we wanted to optimize was:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(x, i) \\in W} (r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda_1\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda_2\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(x, i)$ pairs for which $r_{xi}$ is known. Here we have also introduced two regularization terms to help us with overfitting where $\\lambda_1$ and $\\lambda_2$ are hyper-parameters that control the strength of the regularization.\n",
    "\n",
    "Naturally optimizing with gradient descent involves computing the gradient of the loss function $\\mathcal{L}$ w.r.t. to the parameters.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{p}_x} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{q}_i\\;, ~~~\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{q}_i} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{p}_x \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial(\\lambda_1{\\left\\lVert \\mathbf{p}_x \\right\\rVert}^2)}{\\partial \\mathbf{p}_x} = 2 \\lambda_1 \\mathbf{p_x} \\;, ~~~\n",
    "\\frac{\\partial(\\lambda_2{\\left\\lVert \\mathbf{q}_i \\right\\rVert}^2)}{\\partial \\mathbf{q}_i} = 2 \\lambda_2 \\mathbf{q_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_gradient_descent(M, non_zero_idx, k, val_idx, val_values, \n",
    "                                   reg_lambda, learning_rate, batch_size=-1,\n",
    "                                   max_steps=50000, init='random',\n",
    "                                   log_every=1000, patience=20,\n",
    "                                   eval_every=50):\n",
    "    '''\n",
    "    input\n",
    "        M [N,D] -> matrix to factorize\n",
    "        \n",
    "        non_zero_idx -> indices of non zero entries\n",
    "        \n",
    "        k -> latent dimensions\n",
    "        \n",
    "        Val_idx -> validation indices\n",
    "        \n",
    "        val_values -> validation values\n",
    "        \n",
    "        reg_lambda -> regularization strength\n",
    "        \n",
    "        learning_rate -> sgd learning rate\n",
    "        \n",
    "        batch size -> (mini-) batch size for the sgd\n",
    "        \n",
    "    returns\n",
    "        best_Q -> best Q based on val loss\n",
    "        \n",
    "        best_P -> best P based on val loss\n",
    "        \n",
    "        validation_losses -> to plot loss over time\n",
    "        \n",
    "        train_losses -> to plot loss over time\n",
    "        \n",
    "        converged_after -> convergence point\n",
    "\n",
    "\n",
    "\n",
    "    '''    \n",
    "    #Initialize variables\n",
    "    Q,P = initialize_Q_P(M, k, init=init)  \n",
    "    train_losses, validation_losses, time_per_iteration = [], [], []\n",
    "    best_Q, best_P = Q, P\n",
    "    best_val = loss_validation(val_values, val_idx, P, Q)\n",
    "    grad_P, grad_Q = np.zeros_like(P), np.zeros_like(Q)\n",
    "    converged_after = -1\n",
    "    min_reached = False\n",
    "    \n",
    "    \n",
    "    # get nonzero rows/cols\n",
    "    nz_rows_full, nz_cols_full = non_zero_idx[:,0], non_zero_idx[:,1]\n",
    "    \n",
    "    # compute scaling factor (just for non-fullbatch)\n",
    "    N = non_zero_idx.shape[0]\n",
    "    if batch_size == -1:\n",
    "        b_scaling = 1.\n",
    "    else:\n",
    "        b_scaling = N/batch_size\n",
    "    \n",
    "    start = time.time()\n",
    "    #start of the stochastic gradient descent\n",
    "    for iters in range(max_steps):\n",
    "        if iters % eval_every == 1 or eval_every == 1: start = time.time()\n",
    "            \n",
    "        # select batch indices\n",
    "        if batch_size == -1:\n",
    "            nz_rows_batch, nz_cols_batch = nz_rows_full, nz_cols_full\n",
    "        else:\n",
    "            batch_indices = np.random.choice(N, batch_size, replace=False)\n",
    "            nz_rows_batch, nz_cols_batch = nz_rows_full[batch_indices], nz_cols_full[batch_indices]\n",
    "            \n",
    "        # Get the error matrix\n",
    "        if batch_size == -1:\n",
    "            eps = np.asarray(M[nz_rows_batch,nz_cols_batch] - (Q.dot(P))[nz_rows_batch,nz_cols_batch]).flatten()\n",
    "        else:\n",
    "            eps = np.asarray(M[nz_rows_batch,nz_cols_batch] - (Q[nz_rows_batch].dot(P[:,nz_cols_batch])).diagonal()).flatten()\n",
    "        eps_sparse = sp.csr_matrix((eps, (nz_rows_batch,nz_cols_batch)), shape=M.shape)\n",
    "            \n",
    "        # Compute the gradient\n",
    "        grad_P = (b_scaling*2*eps_sparse.T.dot(Q)).T-reg_lambda*P\n",
    "        grad_Q = b_scaling*2*eps_sparse.dot(P.T)-reg_lambda*Q\n",
    "        # Perform the update\n",
    "        P = P + learning_rate*grad_P\n",
    "        Q = Q + learning_rate*grad_Q\n",
    "\n",
    "        # evaluation\n",
    "        if (iters % eval_every) == 0:\n",
    "            train_losses.append(loss_function(P, Q, M, non_zero_idx))\n",
    "            validation_losses.append(loss_validation(val_values, val_idx, P, Q))\n",
    "            end = time.time()\n",
    "            if iters == 0:\n",
    "                time_per_iteration.append(end - start)\n",
    "            else:\n",
    "                time_per_iteration.append((end - start) / eval_every)\n",
    "            now_at = int(iters//eval_every)\n",
    "            if iters > 1 and validation_losses[now_at] < best_val: \n",
    "                best_Q, best_P, best_val = Q, P, validation_losses[now_at]\n",
    "                converged_after = iters\n",
    "                min_reached = True\n",
    "            elif min_reached:\n",
    "                patience -= 1\n",
    "                \n",
    "        # logging\n",
    "        if (iters % log_every) == 0:\n",
    "            print(\"Iteration %i,  training loss: %.3f, validation loss: %.3f, time per iteration: %.3f\"\n",
    "                 % (iters, train_losses[-1], validation_losses[-1], time_per_iteration[-1]))\n",
    "        if patience == 0: break\n",
    "        if iters == max_steps - 1: converged_after = -1\n",
    "\n",
    "    # final results\n",
    "    average_time = np.mean(np.array(time_per_iteration))\n",
    "    if converged_after == -1:\n",
    "        converged_after = max_steps\n",
    "        print(\"Did not converge in %i iterations, on average %.3f seconds per iteration.\" % (max_steps, average_time))\n",
    "    else:\n",
    "        print(\"Converged after %d iterations, on average %.3f seconds per iteration.\" % (converged_after, average_time))\n",
    "\n",
    "            \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_g_sweep, P_g_sweep, val_l_g_sweep, tr_l_g_sweep, conv_g_sweep =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                                   val_values=val_values_shifted, \n",
    "                                                                                                   reg_lambda=1e-2, learning_rate=1e-3,\n",
    "                                                                                                   init='svd', batch_size=-1,\n",
    "                                                                                                   max_steps=10000, log_every=20, \n",
    "                                                                                                   eval_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.arange(0, len(val_l_g_sweep), 1)\n",
    "plt.plot(x, val_l_g_sweep)\n",
    "plt.title(s='Gradient descent validation loss', loc='center')\n",
    "plt.xlabel(s='Training iteration')\n",
    "plt.ylabel(s='Validation loss')\n",
    "plt.xticks(range(0,len(x), 2))\n",
    "\n",
    "# second plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, tr_l_g_sweep)\n",
    "plt.title(s='Gradient descent training loss', loc='center')\n",
    "plt.xlabel(s='Evaluations')\n",
    "plt.ylabel(s='Training loss')\n",
    "plt.xticks(range(0,len(x), 2))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_g_st, P_g_st, val_l_g_st, tr_l_g_st, conv_g_st = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1e-4, learning_rate=1e-6,\n",
    "                                                                                   init='svd', batch_size=1,\n",
    "                                                                                   max_steps=20000, log_every=500, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.arange(0, len(val_l_g_st), 1)\n",
    "plt.plot(x, val_l_g_st)\n",
    "plt.title(s='Gradient descent validation loss', loc='center')\n",
    "plt.xlabel(s='Training iteration')\n",
    "plt.ylabel(s='Validation loss')\n",
    "plt.xticks(range(0,len(x), 2))\n",
    "\n",
    "# second plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, tr_l_g_st)\n",
    "plt.title(s='Gradient descent training loss', loc='center')\n",
    "plt.xlabel(s='Evaluations')\n",
    "plt.ylabel(s='Training loss')\n",
    "plt.xticks(range(0,len(x), 2))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "Machine learning models are often heavily dependent on the hyperparameter settings, e.g. the learning rate. Here, we will try a simple random search to find good values of the latent factor dimension $k$, the batch size, learning rate, and regularization.  \n",
    "\n",
    "Perform a hyperparameter search to find good values for the batch size, lambda, learning rate, and latent dimension. \n",
    "\n",
    "* For the batch size, we evaluate all values in [1, 32, 512, -1] (-1 corresponds to full-sweep gradient descent).\n",
    "* For $\\lambda$, we randomly sample three values in the interval [0, 1).\n",
    "* For the learning rate, we evaluate all values in [1, 0.1, 0.01].\n",
    "* For the latent dimension, we uniformly sample three values in the interval [5,30].\n",
    "\n",
    "We perform an exhaustive search among all combinations of these values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(M_train, val_idx, val_values):\n",
    "    '''\n",
    "    input\n",
    "        M_train-> matrix to train\n",
    "        \n",
    "        val_idx -> indices for valdation\n",
    "        \n",
    "        val_values -> values for validation\n",
    "    \n",
    "    return\n",
    "        \n",
    "        best_conf -> best performing hyperparameter combination\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #basically the same as for Neural networks\n",
    "    # Store away the nonzero indices of M before subtracting the row means.\n",
    "    nonzero_indices = np.vstack((M_train.nonzero()[0], M_train.nonzero()[1])).T\n",
    "\n",
    "    # Remove user means.\n",
    "    M_shifted, user_means = shift_user_mean(M_train)\n",
    "\n",
    "    # Apply the same shift to the validation and test data.\n",
    "    val_values_shifted = val_values - user_means[val_idx[0]]\n",
    "    test_values_shifted = test_values - user_means[test_idx[0]]\n",
    "    \n",
    "    \n",
    "    batch_sizes = [1,32,512,-1]\n",
    "    lam = np.random.rand(3)*1e-2\n",
    "    lr = [1e-3, 1e-4, 1e-6]\n",
    "    ld = np.random.randint(low=5, high=31, size=3)\n",
    "    best_val_loss = -1\n",
    "    best_conf = {}\n",
    "    for bs in batch_sizes:\n",
    "        for l in lam:\n",
    "            for eta in lr:\n",
    "                for dim in ld:\n",
    "                    print(\"Training with configuration: batch size %i, lambda %f, eta %f, latent dimension %i\"%(bs, l, eta, dim))\n",
    "                    \n",
    "                    ##Train latent_factor_gradient_descent\n",
    "                    Q, P, val_loss, tr_loss, conv =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=dim, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=l, learning_rate=eta,\n",
    "                                                                                   init='svd', batch_size=bs,\n",
    "                                                                                   max_steps=10000, log_every=500, \n",
    "                                                                                   eval_every=20)\n",
    "                    \n",
    "                    val_loss_min = min(val_loss)\n",
    "                    if best_val_loss == -1 or best_val_loss > val_loss_min:\n",
    "                        best_val_loss = val_loss_min\n",
    "                        best_conf = {'batch_size':bs, 'lambda':l, 'learning_rate':eta, 'latent_dimension':dim}\n",
    "                        print(\"New best configuration with loss %f\"%val_loss_min)\n",
    "                    print(\"\\n\")\n",
    "                        \n",
    "        \n",
    "    print(\"Best configuration is {}\").format(best_conf)\n",
    "    return best_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configuration = parameter_search(M_train, val_idx, val_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs = best_configuration['batch_size']\n",
    "blmbd = best_configuration['lambda']\n",
    "blr = best_configuration['learning_rate']\n",
    "bld = best_configuration['latent_dimension']\n",
    "print(\"Best batch size: %i\\nBest lambda: %f\\nBest learning rate: %f\\nBest latent dimension: %i\"%(bbs, blmbd, blr, bld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of gradient descent and alternating optimization\n",
    "\n",
    "* explain whats the advantage of sgd  and the difference between fbgd and mbgd\n",
    "   Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima.\n",
    "* compare results properly (maybe run some more simulations) of same values for k and the RMSE of the different approaches ?**Jorge**\n",
    "\n",
    "\n",
    "\n",
    "* WecCompare the root mean square errors (RMSE) for the training, validation, and test sets different settings of $k$ for both alternating optimization and gradient descent.\n",
    "  While varying the value for k, we can observe that a big k improves the loss on the training set but results in overfitting because bigger matrices Q and P can capture the data while training better. For Gradient Descent we found the best value for k to be around 25, for Alternating Optimization 100 produced the best results.\n",
    "\n",
    "* We compare the test RMSE for the alternating optimization model and the gradient descent model. \n",
    "\n",
    "  We found that the RMSE for gradient descent is lower than the one for alternating optimization. We also found from looking at the plots that higher ratings are predicted more accurately. Thats probably that the stochastic gradient descent do not end up in a local minimum trough the noise enabled because we dont use all the data and a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(P,Q,M,non_zero_idx):\n",
    "    return np.sqrt(np.mean(np.array(M[non_zero_idx[:,0], non_zero_idx[:,1]]-(Q.dot(P))[non_zero_idx[:,0], non_zero_idx[:,1]])**2))\n",
    "\n",
    "def rmse_val(val_values, val_idx, P, Q):\n",
    "    return np.sqrt(np.mean(np.array(val_values-(Q.dot(P))[val_idx[0], val_idx[1]])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best k for gradient descent\n",
    "k_values = [25, 50, 100, 150]\n",
    "best_val_loss = -1\n",
    "best_GD_conf = {}\n",
    "for k in k_values:\n",
    "    print(\"Training gradient descent with k = %i\"%(k))\n",
    "                    \n",
    "    ##Train latent_factor_gradient_descent\n",
    "    Q, P, val_loss, tr_loss, conv =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                    k=k, val_idx=val_idx,\n",
    "                                                                    val_values=val_values_shifted, \n",
    "                                                                    reg_lambda=1e-2, learning_rate=1e-3,\n",
    "                                                                    init='svd', batch_size=-1,\n",
    "                                                                    max_steps=5000, log_every=1000, \n",
    "                                                                    eval_every=20)\n",
    "                    \n",
    "    val_loss_min = min(val_loss)\n",
    "    if best_val_loss == -1 or best_val_loss > val_loss_min:\n",
    "        best_val_loss = val_loss_min\n",
    "        best_GD_conf = {'k':k, 'Q':Q, 'P':P}\n",
    "        print(\"New best configuration with loss %f\"%val_loss_min)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the error of the best GD configuration\n",
    "Q_GD, P_GD = best_GD_conf['Q'], best_GD_conf['P']\n",
    "train_err = rmse(P_GD, Q_GD, M_shifted, nonzero_indices)\n",
    "val_err = rmse_val(val_values_shifted, val_idx, P_GD, Q_GD)\n",
    "test_err = rmse_val(test_values_shifted, test_idx, P_GD, Q_GD)\n",
    "print(\"Training RMSE of best gradient descent model: %f\"%train_err)\n",
    "print(\"Validation RMSE of best gradient descent model: %f\"%val_err)\n",
    "print(\"Test RMSE of best gradient descent model: %f\"%test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best k for alternating optimization\n",
    "k_values = [25, 50, 100, 150]\n",
    "best_val_loss = -1\n",
    "best_AO_conf = {}\n",
    "for k in k_values:\n",
    "    print(\"Training alternating optimization with k = %i\"%(k))\n",
    "                    \n",
    "    ##Train latent_factor_gradient_descent\n",
    "    Q, P, val_loss, tr_loss, conv  = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                            k=k, val_idx=val_idx,\n",
    "                                                                            val_values=val_values_shifted, \n",
    "                                                                            reg_lambda=1, init='random',\n",
    "                                                                            max_steps=100, patience=10)\n",
    "                    \n",
    "    val_loss_min = min(val_loss)\n",
    "    if best_val_loss == -1 or best_val_loss > val_loss_min:\n",
    "        best_val_loss = val_loss_min\n",
    "        best_AO_conf = {'k':k, 'Q':Q, 'P':P}\n",
    "        print(\"New best configuration with loss %f\"%val_loss_min)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the error of the best AO configuration\n",
    "Q_AO, P_AO = best_AO_conf['Q'], best_AO_conf['P']\n",
    "train_err = rmse(P_AO, Q_AO, M_shifted, nonzero_indices)\n",
    "val_err = rmse_val(val_values_shifted, val_idx, P_AO, Q_AO)\n",
    "test_err = rmse_val(test_values_shifted, test_idx, P_AO, Q_AO)\n",
    "print(\"Training RMSE of best alternating optimization model: %f\"%train_err)\n",
    "print(\"Validation RMSE of best alternating optimization model: %f\"%val_err)\n",
    "print(\"Test RMSE of best alternating optimization model: %f\"%test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot pred. vs ground truth\n",
    "\n",
    "In the following scatter plots we can see, how the different ratings get predicted compared to different datasets. \n",
    "**jorge** down here i compare test sets with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user means have to be added to retrieve original values\n",
    "M_pred_AO = Q_AO.dot(P_AO) + np.matrix(user_means).T\n",
    "plt.scatter(np.asarray(M[nonzero_indices[:,0], nonzero_indices[:,1]]).flatten(), np.asarray(M_pred_AO[nonzero_indices[:,0], nonzero_indices[:,1]]).flatten(), alpha=0.2)\n",
    "plt.xlabel(s=\"Ground truth training rating\")\n",
    "plt.ylabel(s=\"Predicted rating\")\n",
    "plt.title(\"Train ratings vs predictions by alternating optimization\", loc='center')\n",
    "plt.ylim([1,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we see that the train ratings match quit well with a standard deviance of around 1 to the predicted ratings of the alternating optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_pred_GD = Q_GD.dot(P_GD) + np.matrix(user_means).T \n",
    "plt.scatter(np.asarray(M[nonzero_indices[:,0], nonzero_indices[:,1]]).flatten(), np.asarray(M_pred_GD[nonzero_indices[:,0], nonzero_indices[:,1]]).flatten(), alpha=0.2)\n",
    "plt.xlabel(s=\"Ground truth training rating\")\n",
    "plt.ylabel(s=\"Predicted rating\")\n",
    "plt.title(\"Training ratings vs predictions by gradient descent\", loc='center')\n",
    "plt.ylim([1,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the gradient descent against the training data we have a larger variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_pred_AO = Q_AO.dot(P_AO) + np.matrix(user_means).T \n",
    "plt.scatter(test_values, np.asarray(M_pred_AO[test_idx]).flatten(), alpha=0.2)\n",
    "plt.xlabel(s=\"Ground truth test rating\")\n",
    "plt.ylabel(s=\"Predicted rating\")\n",
    "plt.title(\"Test ratings vs predictions by alternating optimization\", loc='center')\n",
    "plt.ylim([1,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ratings vs predictions of the alternating optimization leads to good results for high values, but bad for low values (probably because there is more good rated restaurants then bad rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_pred_GD = Q_GD.dot(P_GD) + np.matrix(user_means).T\n",
    "plt.scatter(test_values, np.asarray(M_pred_GD[test_idx]).flatten(), alpha=0.2)\n",
    "plt.xlabel(s=\"Ground truth test rating\")\n",
    "plt.ylabel(s=\"Predicted rating\")\n",
    "plt.title(\"Test ratings vs predictions by gradient descent\", loc='center')\n",
    "plt.ylim([1,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the test vs predictions of the sgd method our results are far better, but all quit close to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
