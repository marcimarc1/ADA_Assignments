{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: \"Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1: Marc Schmid; Student Number: 13349752\n",
    "\n",
    "E-mail: marc.steffen.schmid@gmail.com\n",
    "\n",
    "Github: https://github.com/marcimarc1/ADA_Assignments\n",
    "\n",
    "This is a Review, so all intelectual properties and thoughts in content belong to the authors of the original papers.\n",
    "\n",
    "(Including models and pictures, but not the Coco Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this work I aim to review the paper \"Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer\" written by X. Wang, G. Oxholm, D. Zhang, Y. Wang. I will summarize the main topics and ideas, as well as short excursions to unknown neural network structures, where the paper prerequisites knowledge not covered in the lecture yet.\n",
    "I will analyse the innovation the work brings to neural networks and computer vision. Afterwards I am going to review the technical quality of the paper and search for some application of the algorithms. Finally I will describe the quality of the presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The paper is of the area of deep learning for computer vision. It investigates new generative neural network models to transfer artistic styles onto arbitrary images. There are already online learning algorithms, but they do not represent the style transfer correctly as they fail to display the style in local regions of the picture. The paper is mainly based on the work of L. A. Gatys, A. S. Ecker, and M. Bethge.\n",
    "![Overall Architecure of the Neural Network](firstPic.png \"Comparison of the different Neural Network Architectures by\")\n",
    "Top row: (a) The style guide is \"At the Close of Day\" by Tomas King, and (f) is the content image. (b) is the result of Gatys et alâ€™s\n",
    "optimization-based method. (Result size is 512 due to memory limitation of the method) (c), (d) and (e) are results generated by different\n",
    "feed-forward networks (all are of size 1024). Bottom row: the zoom-in display of the regions enclosed in the red boxes from the top row.\n",
    "As can be seen, all results are repainted with the colour of the style image. However, a closer examination shows that the brush strokes are\n",
    "not captured well in (c) and (d). The zoom-in region in (b) is a little blurry. Comparing with the others, our multimodal transfer (e) is\n",
    "capable of simulating more closely the brushwork of the original artwork on high-resolution images.(From [1])\n",
    "\n",
    "\n",
    "Image style transfer uses convolutional neural networks, where a pre-trained deep learning network for visual recognition is used to capture both style and content representations. They reduce the computing time of the neural network by training it offline and introducing a new hierarchical deep convolutional neural network architecture. This helps to fasten the computation of style transfers in software like Adobe Photoshop. Through the fully convolutional network it is guaranteed to process pictures of different sizes, however it has to have a minimal size, otherwise a 1 by 1 convolution will eventually crash somewhere in the neural net. They introduce a new structure in the first neural network they use, where they differentiate in colour scheme and illumination, as visual perception is far more sensitive to changes in illumination than in colour. They prove in experiments that they achieve better results for the style transfer than in traditional CNN-Architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "![Overall Architecure of the Neural Network](overarch.png \"Overall Architecure of the Neural Network\")\n",
    "The overall architecture of the neural network is structured into 3 subnets (MT Network). The Style Subnet, the Enhance Subnet and the Refine Subnet. This network architecture was chosen because its difficult to optimally adjust textures to a variety of styles. The network takes an image as input and generates an output image for each subnet with increasing resolution. These output images are then taken as inputs to the loss network to calculate a stylization loss for each image. Afterwards the total loss is a weighted combination of all stylization losses. The subnet prediction is defined as \n",
    "$$X_k= f(\\omega_k, x)$$, where $X_k$ is the prediction of the k-th network, $\\omega_k$ are the parameters of the k-th network, $x$ is the input image and $f()$ is the corresponding neural network architecture. \n",
    "\n",
    "### Style Subnet\n",
    "As visual perception is far more sensitive to illuminance than colour, the Style Subnet addresses the three colour channels (in the RGB-Block) as well as the luminance channels (in the L-Block). The feature maps computed by those both nets are than concatenated in the depth dimension and further processed by the following Convolution Block. The goal of the Style Subnet is to preserve the content and to adjust the textures to the style a lot. The RGB and L Block consist of three convolutional layers followed by three instance normalization layers and ReLU activation functions respectively, followed by three residual blocks. After the concatenation of the two blocks there follows an up-sampling convolutional layer structure to reach the right size of the image.\n",
    "\n",
    "### Enhance Subnet and Refine Subnet\n",
    "\n",
    "The job of the Enhance Subnet and Refine Subnet is to further change the style of the picture, but less than the Style Subnet. The Enhance Subnet as well as the Refine Subnet are fully convolutional neural networks which up-sample the images to the size of 512 and 1024 respectively. The Enhance Subnet consists of 4 convolutional layers, 6 residual blocks, followed by an up-sampling convolutional structure. The Refine Subnet is embodied similar with 4 convolutional layers, 3 residual blocks as well an up-sampling structure, so that the image matches the target.\n",
    "\n",
    "#### Comparison of the Refinement\n",
    "\n",
    "![Subent Structure Refinement](refstruc.png \"Display the absolute and relative changes done by the stylisation\")\n",
    "(a) is the content image and (b) is the style image.\n",
    "(c)(d)(e) show the outputs of the three subnets, $\\hat{y}_1$, $\\hat{y}_2$ and $\\hat{y}_3$,\n",
    "whose sizes are 256, 512 and 1024 respectively. The third row\n",
    "depicts the absolute difference between: (f) the content image and\n",
    "the output image $\\hat{y}_1$, (g) the output images $\\hat{y}_1$ and $\\hat{y}_2$, and (h) the\n",
    "output images $\\hat{y}_2$ and $\\hat{y}_3$.(From [1])\n",
    "\n",
    "We can see that the total difference between the pictures is decreasing in each subnet. So that the Style Subnet changes the content picture a lot, while the enhance and refine subnets focus on detail adaption.\n",
    "\n",
    "## Mathematical Foundations and Learning Schemes\n",
    "\n",
    "As explained in the network architecture, the multimodal CNN takes an imput image and returns 3 output images. These output images are the put into the loss network which calculates stylization losses. The loss network is the VGG16 (in the implementation I used the pretrained model from torchvision).\n",
    "\n",
    "\n",
    "#### Excursion: VGG16-NN Structure and use as Stylization Loss Network\n",
    "\n",
    "![VGG16 Structure](vgg16_struct.png 'Structure of the VGG 16')\n",
    "\n",
    "The VGG16 Network was actually build for 224x224x3 pictures as a classification task. It consists of twelve convolutional layers with max pooling layers inbetween which extend to 512 channels in the last convolution , before a few fully connected layers are used for classification. In the MT Network the VGG16-Loss is expanded to a Content Loss and Texture/Style Loss.\n",
    "\n",
    "### Loss Function\n",
    "![VGG16 Loss Network](loss_net.png 'Structure of the VGG 16 loss network')\n",
    "\n",
    "#### Content Loss \n",
    "Let $\\vec{p}$ and $X$ be the original image and the generated image, and $P^l$ and $F^l$ their respective feature representation in layer l. The squared-error loss between the two feature representations is defined as\n",
    "\\begin{equation}\n",
    " \\mathcal{L}_{content}(\\vec{p}, X, l ) = \\frac{1}{2} \\sum_{i,j}(F^l_{i,j}-P^l_{i,j})^2\n",
    "\\end{equation}\n",
    "Thus, the content loss compares the feature difference for the \\textit(l)-th layer of the convolution of the VGG16. It is a good idea to compute the content loss in one of the last layers of the VGG net, as in the first convolutional layers the standard features like vertical and horizontal lines are covered. The last convolution layers are more complex features get covered, so that the content really gets compared, and not some superficial features.\n",
    "\n",
    "#### Style Loss\n",
    "The Style loss consists of the correlations between the different filter responses, where the expectation is taken over the spatial extent of the feature\n",
    "maps. These feature correlations are given by the Gram matrix $G^l \\in \\mathcal{R}^{N_l\\times N_l}$, where $G^l_{ij}$ is the inner product between the vectorised feature maps $i$ and $j$ in layer $l$:\n",
    "$$G^l_{i,j}= \\sum_k F^l_{ik}F^l_{j,k}$$\n",
    "which equals:\n",
    "\\begin{equation}\n",
    "G(x_1,\\dots, x_n)=\\begin{pmatrix} \\langle x_1,x_1\\rangle & \\langle x_1,x_2\\rangle &\\dots & \\langle x_1,x_n\\rangle\\\\\n",
    " \\langle x_2,x_1\\rangle & \\langle x_2,x_2\\rangle &\\dots & \\langle x_2,x_n\\rangle\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    " \\langle x_n,x_1\\rangle & \\langle x_n,x_2\\rangle &\\dots & \\langle x_n,x_n\\rangle\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "in matrix notation.\n",
    "By including the feature correlations of multiple layers, a stationary, multi-scale representation of the input image is obtained, which captures its texture information but not the global arrangement.\n",
    "Therefore the texture function loss is defined as \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{L}_{texture}(\\vec{q}, X) = \\sum_{lâˆˆL}w_l(G^l(\\vec{q})-G^l(X))^2\n",
    "\\end{equation}\n",
    "\n",
    "with the amount of Layers chosen $L$, generated picture $X$ and style target  $\\vec{q}$, and a chosen weight for each layer $w_l$.\n",
    "\n",
    "Now it is up to the user to decide which weight of content and texture loss is \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{style}(\\vec{p},\\vec{q}, X) =\\alpha \\mathcal{L}_{content}(\\vec{p}, X, l ) +\\beta \\mathcal{L}_{texture}(\\vec{q}, X)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are the weights of the content loss and texture\n",
    "loss, respectively.\n",
    "\n",
    "#### Hirarchical Stylization Loss Function\n",
    "As the previously discussed part is nothing new, as it got already presented at [2] the MT Network provides 3 outputs $X_k$ with $ kâˆˆ[1,2,3]$ for the MT Network. Therefore multiple loss function get computed which have to be combined. Thus, the hierarchical stylization loss function $\\mathcal{L}_H $ is introduced. The loss function is a sum of weighted loss function of the stylization losses:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_H = \\sum_k \\lambda_k \\mathcal{L}_{style}(\\vec{p}_k,\\vec{q}_k, X_k) \n",
    "\\end{equation}\n",
    "where $\\lambda_k$ is the weight of the corresponding stylization loss.\n",
    "\n",
    "To train the neural network, it is important to compute a gradient for the backward pass. For each subnet ($k$) feature space $\\omega_k$ is trained to minimize the parallel weighted stylization losses that are computed from the outputs of the layer after the the layer of which the actuall loss is taken.\n",
    "Thus, they defined the features as \n",
    "\n",
    "$$\\omega_k=argmin_{\\omega_k}E_x\\left[\\sum_{i\\geq k} \\lambda_i \\mathcal{L}^k_{style}(f(\\omega_k,x),\\vec p ,\\vec q) \\right]$$\n",
    "The gradients get computed by\n",
    "\n",
    "$$\\delta \\omega_k=\\begin{cases}\n",
    "(lambda_k \\mathcal{L}^k_{style})'  \\  & \\text{if }k=K\\\\\n",
    "(lambda_k \\mathcal{L}^k_{style}, \\delta \\omega_{k+1})'  \\  & \\text{if} 1 \\leq k \\leq K\\end{cases}\\ $$\n",
    "\n",
    "F. Meissen told me to add a regularization loss like in [3] for better training results and to prevent overfitting, as I haven't trained on the full Coco dataset, just on 50% of the dataset.\n",
    "\n",
    "\n",
    "## Used Layers\n",
    "(Just a short mention as they were not explained in the lecture yet)\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "![Convolutional Layers](cnn.jpeg \"Display a convolutional downsampling Structure\")\n",
    "In this figure we can see a downsampling structure of a convolution just by different filters and strides which decreases the width and height. The amount of filters increases the depth.\n",
    "\n",
    "In convolutional layers we use so called filters to connect the neurons. The neurons are not all to all connected anymore, either they are dependent on their neighbours. In convolutions we use the fact that pixels next to each other are somehow connected to each other. Through weight sharing in convolutional layers \n",
    "\n",
    "![Convolutional Filters](conv_filter.png \"Display a convolutional filter passing\")\n",
    "In this figure we can see how two 3x3 filters/kernels work on an input volume of 7x7x3 with padding 1. You swipe the kernels over the different depth levels. In general you start at the upper left and take one step to the right until you reach the other border. Then you slip one line down until you covered the whole picture.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "##### Padding \n",
    "The padding p puts some numbers around of the border of the picture. With it we can change the size of the output size.\n",
    "To keep the size it is good to use stride one and the formula for the padding is \n",
    "$$ p = \\frac{k-1}{2}$$ with k as amount of pixels in one direction.\n",
    "\n",
    "##### Spatial Extend\n",
    "The spatial extend F is the receptive field size of the convolution layer neurons.\n",
    "\n",
    "##### Stride\n",
    "The stride s specifies how many pixels we move the filter in one step. Stride 1 is common. But to reduce size of the convolutional output, stride 2 is also used quit often.\n",
    "\n",
    "##### Computation of the Output Size\n",
    "\n",
    "The output size of the new convolutional volume can be computed, if the stride, spatial extend, padding and the input width and hight are known:\n",
    "\n",
    "$$W_2= \\frac{W_1-F+2P}{S}+1$$\n",
    "$$H_2= \\frac{H_1-F+2P}{S}+1$$\n",
    "\n",
    "The Depth is just the amount of kernels used in the layer.\n",
    "\n",
    "\n",
    "### Residual Block\n",
    "Deep neural networks suffer from vanishing or exploding gradient as well as saturation of accuracy, dependant on the activation functions.\n",
    "Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution to the deeper model by construction: the layers are copied from the learned shallower model, and the added layers are identity mapping. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart.\n",
    "![Residaul Block](res_struc.png \"Residual structure\")\n",
    "\n",
    "Instead of hoping each stack of layers directly fits a desired underlying mapping, we explicitly let these layers fit a residual mapping. The original mapping is recast into $F(x)+x$. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\n",
    "\n",
    "### Instance Normalization\n",
    "Normalization is used to keep the values of the input in zero mean, unit variance to improve the performance of neural networks. With sigmoid-like activation functions we tent to saturate quickly for large values in input, which contributes to the vanishing gradient problem. That's why the batch normalization was so successful.\n",
    "Instance normalization is basically batch normalization with the key difference that the latter applies the normalization to a whole batch of images instead of single ones.\n",
    "This prevents instance-specific mean and covariance shift simplifying the learning process, of which is different from batch normalization. Furthermore, the instance normalization layer is applied at test time as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innovation\n",
    "This paper introduces a new hierarchical deep convolutional neural network architecture for offline training. The results get compared to Gatys Paper and different feed-forward neural networks as well as different singular transfer networks. Those networks are other state-of-the-art networks by Johnson: 'Perceptual losses for real-time style transfer and super-resolution', and Ulyanov: 'Instance normalization: The missing ingredient for fast stylization'.\n",
    "Principally they introduce a new loss function, compared to Johnson, Gatsy and Ulyanov and concatenate three networks with the same structure but different depth. In my opinion they improved the neural network structure of the mentioned papers, and combine the results of the paper.\n",
    "\n",
    "\n",
    "Nevertheless, they introduce a very interesting comparison of a singular transfer and a multimodal transfer on high resolution images.\n",
    "The setup of the comparison is comprehensible, as they generated a singular transfer neural network with the same amount of weights as the multimodal transfer neural network. While the singular transfer model either had too much or too little style adaption, the multimodal transfer model has a good balance of both. Thus, as can be seen in the implementation, you are able to adapt how much style and content should be learned in every stage and it increases the accuracy of the wanted output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Quality\n",
    "\n",
    "I would rate the technical quality of the paper good over all. It is well cited and you can find details easily in the references. The Coco Dataset is a well known Dataset in Computer Vision and the work is supported by reputable companies and research institutes.\n",
    "\n",
    "In therms of technical language they sometimes mess around with words, as for beginners in neural networks a feed forward neural network does not need to be of convolutional structure. But as this is a more advanced research paper, most of the people reading it are familiar with those expressions.\n",
    "All the figures are well documented and described, which makes it easy to follow the text and the new thoughts.\n",
    "As the paper is highly linked to Johnson, Gatsy and Ulyanov, it is mandatory to read those papers to fully understand the neural network architecture.\n",
    "As they introduced their new Loss function, it was very hard to follow and understand what they meant by the stylization losses are computed from the latter outputs of the actual target. The function itself is self-explanatory, although it is a highly interleaved function with a lot of different indices, which makes it really hard to implement the new loss.\n",
    "\n",
    "Even if some minor explanations are missing, the implement models and achieved results are reproduceable, since all loss functions, network architecture and the optimization methods are formally described.\n",
    "\n",
    "The experiments and comparisons at the end had always the same assumptions.\n",
    "As they introduce this new multimodal transfer network, it is hard to compare it to other multimodal networks, as they were non-existent at the time, so comparing it to the singular transfer network is the logical step. It would have been a good idea to create more than one multimodal network, as the algorithm is scalable and it's easy to introduce maybe 2-5 subnet models. Therefore a higher resolution of the pictures would have been possible and a more detailed scale of weights.\n",
    "\n",
    "As they compare processing speed and memory usage, they showed that their network is highly advanced (in terms of processing speed and memory usage) to a singular transfer network with the same size, as it has a lower processing time and uses just half of the memory. They also compared it to the Johnson Net you can see that the Johnson net is more efficient in processing speed and memory usage then the MT Net. \n",
    "\n",
    "Unfortunately, they do not explain the training process and explicit implementation hints. Hence it was really helpful that there are altered implementations on github.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application and X-factor\n",
    "\n",
    "The newly introduced Network enables artists and designers to adapt new styles to their work. As the style transfer networks need a lot of memory to safe them it would be good to provide them as a website or on a cloud for memory efficiency. Simple machine learning algorithms are not able to alter images in this way, so i think that the method is thoroughly appropriated for this topic. As this is a generative neural network model the scientific value is not outstanding to other introductions, but in combination with arts this paper delivers a high value of innovation (in general the whole three to four papers which developed this art an neural network combination).\n",
    "Through my research in implementation of this paper, they already implemented this method not just for image style transfer, but also for video style transfer. Other applications of this neural network architecture may be in the finance sector for stock market transfer predictions (of course not the fully CNN as the datatypes are completely different, but NN-models with multi subnets to improve the results) or even in production planning as this neural network may generate new plant designs or even optimize old plants by redesigning them (of course with different training sets and altering of the layer architecture).\n",
    "\n",
    "Currently a lot of energy is put into neural network research. However, I cannot think of any other application, where this explicit neural network can be used. As neural networks are a emergent field in computerscience, this work contributes to the wide range of applications for neural networks.\n",
    "\n",
    "New research in generative models can be used for graphs. A research group from the TU Munich introduced the first implicit generative model for graphs able to mimic real-world networks in March 2018 (https://arxiv.org/abs/1803.00816).\n",
    "I am really looking forward to see more about generative models, as they are not just use supervised learning but also introduce new knowledge to mankind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation\n",
    "\n",
    "The paper was easy to understand in general, but there were some difficulties regarding the details of the other papers. While they talked a lot about the loss functions which were already introduced in detail in Gatsy [2], it was hard to implement the instance normalization. As the Johnson [3] paper was really short, they should have picked up the topic again, as they used the instance normalization layer in every subnet. More detailed experiments would be great, but they actually explained their insights well.\n",
    "The pictures were consistent and also the visualization was well reasoned. I think a picture should be relatively self explaining. I liked that at Figure 1,4,5. However, at the other pictures they refer to chapters of their work. This is a no go, as the pictures should support the chapters and should not be harder to understand without reading it. I suppose that they should change the caption of those figures to something more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] X. Wang, G. Oxholm, D. Zhang, Y. Wang. Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer.  Accepted by CVPR 2017\n",
    "\n",
    "[2] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer\n",
    "using convolutional neural networks. In Proceedings of the\n",
    "IEEE Conference on Computer Vision and Pattern Recognition,\n",
    "pages 2414â€“2423, 2016.\n",
    "\n",
    "[3] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for\n",
    "real-time style transfer and super-resolution. In European\n",
    "Conference on Computer Vision, pages 694â€“711. Springer,\n",
    "2016.\n",
    "\n",
    "[4] CS231n: Convolutional Neural Networks for Visual Recognition, Stanford 2018\n",
    "\n",
    "[5] Github Implementation Help: https://github.com/ceshine/fast-neural-style\n",
    "\n",
    "[6] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization:\n",
    "The missing ingredient for fast stylization. arXiv\n",
    "preprint arXiv:1607.08022, 2016.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding of the topic I like to implement the code. I commented the code, if I used existing implementations.\n",
    "Some code is based on the fast neural style transfer implementation by CeShine Lee and Felix Meissen [ https://github.com/ceshine/fast-neural-style ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First constructed  different neural network modules to keep the code readable. The references are commented in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\" ConvBlock\n",
    "    simple convolution with reflection padding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = int(np.floor(kernel_size / 2))\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\" ResidualBlock\n",
    "    introduced in: https://arxiv.org/abs/1512.03385\n",
    "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResizeConvLayer(nn.Module):\n",
    "    \"\"\" ResizeConvLayer\n",
    "    upsampling with Nearest neighbor interpolation and a conv layer\n",
    "    to avoid checkerboard artifacts.\n",
    "    ref: https://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, scale_factor=2):\n",
    "        super(ResizeConvLayer, self).__init__()\n",
    "        reflection_padding = int(np.floor(kernel_size / 2))\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.nearest_neighbor = nn.Upsample(scale_factor=scale_factor, mode='nearest')\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        out = self.nearest_neighbor(x_in)\n",
    "        out = self.reflection_pad(out)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "class InstanceNormalization(nn.Module):\n",
    "    \"\"\"InstanceNormalization\n",
    "    Improves convergence of neural-style.\n",
    "    ref: https://arxiv.org/pdf/1607.08022.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, eps=1e-9):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor(dim))\n",
    "        self.shift = nn.Parameter(torch.FloatTensor(dim))\n",
    "        self.eps = eps\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        self.scale.data.uniform_()\n",
    "        self.shift.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.size(2) * x.size(3)\n",
    "        t = x.view(x.size(0), x.size(1), n)\n",
    "        mean = torch.mean(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        # Calculate the biased var. torch.var returns unbiased var\n",
    "        var = torch.var(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x) * ((n - 1) / float(n))\n",
    "        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
    "        scale_broadcast = scale_broadcast.expand_as(x)\n",
    "        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
    "        shift_broadcast = shift_broadcast.expand_as(x)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = out * scale_broadcast + shift_broadcast\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first neural subnet (Style Subnet) we need the rgb-, the L- and the Conv-Block. The rgb-Block adresses the utilization of color, while the L-Block adresses the luminance, as the visual perception is much more sensitive to change in luminance than color. The features of the different blocks are the joined together along the depth dimension to be further computed by the Conv-Block. The structure of the net is defined in the initialization of the style subnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleSubnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleSubnet, self).__init__()\n",
    "        # Transform to Grayscale\n",
    "        self.togray = nn.Conv2d(3, 1, kernel_size=1, stride=1)\n",
    "        w = torch.nn.Parameter(torch.tensor([[[[0.299]],\n",
    "                                              [[0.587]],\n",
    "                                              [[0.114]]]]))\n",
    "        self.togray.weight = w\n",
    "\n",
    "        # RGB Block\n",
    "        self.rgb_conv1 = ConvLayer(3, 16, kernel_size=9, stride=1)\n",
    "        self.rgb_in1 = InstanceNormalization(16)\n",
    "        self.rgb_conv2 = ConvLayer(16, 32, kernel_size=3, stride=2)\n",
    "        self.rgb_in2 = InstanceNormalization(32)\n",
    "        self.rgb_conv3 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.rgb_in3 = InstanceNormalization(64)\n",
    "        self.rgb_res1 = ResidualBlock(64)\n",
    "        self.rgb_res2 = ResidualBlock(64)\n",
    "        self.rgb_res3 = ResidualBlock(64)\n",
    "\n",
    "        # L Block\n",
    "        self.l_conv1 = ConvLayer(1, 16, kernel_size=9, stride=1)\n",
    "        self.l_in1 = InstanceNormalization(16)\n",
    "        self.l_conv2 = ConvLayer(16, 32, kernel_size=3, stride=2)\n",
    "        self.l_in2 = InstanceNormalization(32)\n",
    "        self.l_conv3 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.l_in3 = InstanceNormalization(64)\n",
    "        self.l_res1 = ResidualBlock(64)\n",
    "        self.l_res2 = ResidualBlock(64)\n",
    "        self.l_res3 = ResidualBlock(64)\n",
    "\n",
    "        # Residual layers\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        self.res6 = ResidualBlock(128)\n",
    "\n",
    "        # Upsampling Layers\n",
    "        self.rezconv1 = ResizeConvLayer(128, 64, kernel_size=3, stride=1)\n",
    "        self.in4 = InstanceNormalization(64)\n",
    "        self.rezconv2 = ResizeConvLayer(64, 32, kernel_size=3, stride=1)\n",
    "        self.in5 = InstanceNormalization(32)\n",
    "        self.rezconv3 = ConvLayer(32, 3, kernel_size=3, stride=1)\n",
    "\n",
    "        # Non-linearities\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Resized input image is the content target\n",
    "        resized_input_img = x.clone()\n",
    "\n",
    "        # Get RGB and L image\n",
    "        x_rgb = x\n",
    "        with torch.no_grad(): x_l = self.togray(x.clone())\n",
    "\n",
    "        # RGB Block\n",
    "        y_rgb = self.relu(self.rgb_in1(self.rgb_conv1(x_rgb)))\n",
    "        y_rgb = self.relu(self.rgb_in2(self.rgb_conv2(y_rgb)))\n",
    "        y_rgb = self.relu(self.rgb_in3(self.rgb_conv3(y_rgb)))\n",
    "        y_rgb = self.rgb_res1(y_rgb)\n",
    "        y_rgb = self.rgb_res2(y_rgb)\n",
    "        y_rgb = self.rgb_res3(y_rgb)\n",
    "\n",
    "        # L Block\n",
    "        y_l = self.relu(self.l_in1(self.l_conv1(x_l)))\n",
    "        y_l = self.relu(self.l_in2(self.l_conv2(y_l)))\n",
    "        y_l = self.relu(self.l_in3(self.l_conv3(y_l)))\n",
    "        y_l = self.l_res1(y_l)\n",
    "        y_l = self.l_res2(y_l)\n",
    "        y_l = self.l_res3(y_l)\n",
    "\n",
    "        # Concatenate blocks along the depth dimension\n",
    "        y = torch.cat((y_rgb, y_l), 1)\n",
    "\n",
    "        # Residuals\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.res6(y)\n",
    "\n",
    "        # Decoding\n",
    "        y = self.relu(self.in4(self.rezconv1(y)))\n",
    "        y = self.relu(self.in5(self.rezconv2(y)))\n",
    "        y = self.rezconv3(y)\n",
    "\n",
    "\n",
    "        # Clamp image to be in range [0,1] after denormalization\n",
    "        y[0][0].clamp_((0-0.485)/0.299, (1-0.485)/0.299)\n",
    "        y[0][1].clamp_((0-0.456)/0.224, (1-0.456)/0.224)\n",
    "        y[0][2].clamp_((0-0.406)/0.225, (1-0.406)/0.225)\n",
    "\n",
    "        return y, resized_input_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Style Subnet is intended to stylize the input image, but it is hard for the Style Subnet to keep the textures and content of the original image and adjust the image properly to the new style. Therefore, two more nets were introduced with high weights on the style and content respectively to further enhance the stylization. The Enhance Subnet and the Refine Subnet at further details to the image, while doing less in absolute difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhanceSubnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhanceSubnet, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)   # size = 512\n",
    "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)   # size = 256\n",
    "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)   # size = 128\n",
    "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
    "        self.conv4 = ConvLayer(128, 256, kernel_size=3, stride=2)   # size = 64\n",
    "        self.in4 = nn.InstanceNorm2d(256, affine=True)\n",
    "\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(256)\n",
    "        self.res2 = ResidualBlock(256)\n",
    "        self.res3 = ResidualBlock(256)\n",
    "        self.res4 = ResidualBlock(256)\n",
    "        self.res5 = ResidualBlock(256)\n",
    "        self.res6 = ResidualBlock(256)\n",
    "\n",
    "        # Upsampling Layers\n",
    "        self.rezconv1 = ResizeConvLayer(256, 128, kernel_size=3, stride=1)\n",
    "        self.in5 = nn.InstanceNorm2d(128, affine=True)\n",
    "        self.rezconv2 = ResizeConvLayer(128, 64, kernel_size=3, stride=1)\n",
    "        self.in6 = nn.InstanceNorm2d(64, affine=True)\n",
    "        self.rezconv3 = ResizeConvLayer(64, 32, kernel_size=3, stride=1)\n",
    "        self.in7 = nn.InstanceNorm2d(32, affine=True)\n",
    "        self.rezconv4 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
    "\n",
    "        # Non-linearities\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.upsample(X)\n",
    "        # resized input image is the content target\n",
    "        resized_input_img = X.clone()\n",
    "\n",
    "        y = self.relu(self.in1(self.conv1(X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.relu(self.in4(self.conv4(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.res6(y)\n",
    "        y = self.relu(self.in5(self.rezconv1(y)))\n",
    "        y = self.relu(self.in6(self.rezconv2(y)))\n",
    "        y = self.relu(self.in7(self.rezconv3(y)))\n",
    "        y = self.rezconv4(y)\n",
    "\n",
    "        # Clamp image to be in range [0,1] after denormalization\n",
    "        y[0][0].clamp_((0-0.485)/0.299, (1-0.485)/0.299)\n",
    "        y[0][1].clamp_((0-0.456)/0.224, (1-0.456)/0.224)\n",
    "        y[0][2].clamp_((0-0.406)/0.225, (1-0.406)/0.225)\n",
    "\n",
    "        return y, resized_input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefineSubnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefineSubnet, self).__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
    "\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "\n",
    "        # Upsampling Layers\n",
    "        self.rezconv1 = ResizeConvLayer(128, 64, kernel_size=3, stride=1)\n",
    "        self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
    "        self.rezconv2 = ResizeConvLayer(64, 32, kernel_size=3, stride=1)\n",
    "        self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
    "        self.rezconv3 = ConvLayer(32, 3, kernel_size=3, stride=1)\n",
    "\n",
    "        # Non-linearities\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        in_X = X\n",
    "        # resized input image is the content target\n",
    "        resized_input_img = in_X.clone()\n",
    "\n",
    "        y = self.relu(self.in1(self.conv1(in_X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.relu(self.in4(self.rezconv1(y)))\n",
    "        y = self.relu(self.in5(self.rezconv2(y)))\n",
    "        y = self.rezconv3(y)\n",
    "        y = y + resized_input_img\n",
    "\n",
    "        # Clamp image to be in range [0,1] after denormalization\n",
    "        y[0][0].clamp_((0-0.485)/0.299, (1-0.485)/0.299)\n",
    "        y[0][1].clamp_((0-0.456)/0.224, (1-0.456)/0.224)\n",
    "        y[0][2].clamp_((0-0.406)/0.225, (1-0.406)/0.225)\n",
    "\n",
    "        return y, resized_input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For readable code I add some common utility function of pytorch, those are not made by me, they are copied from the Lecture Notes of 'Introduction to Deep Learning' from Technical University of Munich. I can't reference them properly as they are a part of the assignment of the course. There is no solution on github, but it is quit similar to Stanfords CS231 course, for which there are plenty of solutions on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Transform tensor back to frame \"\"\"\n",
    "def recover_frame(frame):\n",
    "    frame = frame.cpu().squeeze(0)\n",
    "    denormalizer = tensor_denormalizer()\n",
    "    frame = denormalizer(frame)\n",
    "    frame.data.clamp_(0, 1)\n",
    "    toPIL = transforms.Compose([transforms.ToPILImage(), transforms.Resize((540, 304))])\n",
    "    frame = toPIL(frame)\n",
    "    return frame\n",
    "\n",
    "\n",
    "\"\"\" Image loader, loads image from file using PIL and converts it to torch tensor \"\"\"\n",
    "def image_loader(image_name, size=512):\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    loader = transforms.Compose([transforms.Resize(size),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 tensor_normalizer()])\n",
    "    image = loader(image).unsqueeze(0)   # If only one image, add a fake dimension in front to augment a batch\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "\"\"\" Image loader for the style image, returns 3 versions with resolutions in sizes_list \"\"\"\n",
    "def style_loader(image_name, device, sizes_list):\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    out = []\n",
    "    for size in sizes_list:\n",
    "        loader = transforms.Compose([transforms.Resize(size),\n",
    "                                     transforms.CenterCrop(size),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     tensor_normalizer()])\n",
    "        style_img = loader(image).unsqueeze(0)\n",
    "        out.append(style_img.to(device, torch.float))\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\" Imshow, displays image using matplotlib \"\"\"\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    denormalizer = tensor_denormalizer()\n",
    "    image = denormalizer(image)\n",
    "    image.data.clamp_(0, 1)\n",
    "    toPIL = transforms.ToPILImage()\n",
    "    image = toPIL(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "\"\"\" Saves image in the /output folder with a specified name as .jpg \"\"\"\n",
    "def save_image(tensor, title=\"output\"):\n",
    "    image = tensor.cpu().clone()  # clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    denormalizer = tensor_denormalizer()\n",
    "    image = denormalizer(image)\n",
    "    image.data.clamp_(0, 1)\n",
    "    toPIL = transforms.ToPILImage()\n",
    "    image = toPIL(image)\n",
    "    scriptDir = os.path.dirname(__file__)\n",
    "    image.save(\"{}.jpg\".format(title))\n",
    "\n",
    "\n",
    "\"\"\" Returns the gram matrix of a feature map \"\"\"\n",
    "def gram_matrix(input):\n",
    "    b, ch, h, w = input.size()\n",
    "\n",
    "    features = input.view(b, ch, h * w)  # change input to vectorized feature map K x N\n",
    "    features_t = features.transpose(1, 2)\n",
    "\n",
    "    # the gram matrix needs to be normalized because otherwise the early layers with a bigger N\n",
    "    # will result in higher values of the gram matrix.\n",
    "    gram = features.bmm(features_t) / (ch * h * w)  # compute the gram matrix bmm = batch matrix-matrix product -> K x K\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n",
    "\"\"\" Transforms to normalize the image while transforming it to a torch tensor \"\"\"\n",
    "def tensor_normalizer():\n",
    "    return transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\"\"\" Denormalizes image to save or display it \"\"\"\n",
    "def tensor_denormalizer():\n",
    "    return transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                               transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the neural network. Therefore, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train imports\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the variables for the style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 1\n",
    "STYLE_NAME = \"picasso\"\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "CONTENT_WEIGHTS = [1, 1, 1]\n",
    "STYLE_WEIGHTS = [2e4, 1e5, 1e3] # Checkpoint single style\n",
    "#STYLE_WEIGHTS = [5e4, 8e4, 3e4] # Checkpoint two styles\n",
    "LAMBDAS = [1., 0.5, 0.25]\n",
    "REG = 1e-7\n",
    "LOG_INTERVAL = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Allow PIL to read truncated blocks when loading images \"\"\"\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Add a seed to have reproducable results \"\"\"\n",
    "\n",
    "SEED = 1080\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configure training with or without cuda \"\"\"\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     torch.cuda.manual_seed(SEED)\n",
    "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#     kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "# else:\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Coco - Dataset.  Download Link-> http://cocodataset.org/#download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load coco dataset \"\"\"\n",
    "\n",
    "print(\"Loading dataset..\")\n",
    "DATASET = 'D:/train2014' # <- Change this line to the path of your coco dataset\n",
    "transform = transforms.Compose([transforms.Resize(IMAGE_SIZE),\n",
    "                                transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                transforms.ToTensor(), tensor_normalizer()])\n",
    "# http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder\n",
    "train_dataset = datasets.ImageFolder(DATASET, transform)\n",
    "# http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "print(\"...Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load Style Image \"\"\"\n",
    "\n",
    "style_img_256, style_img_512, style_img_1024 = style_loader(\n",
    "    \"styles/\" + STYLE_NAME + \".jpg\", device, [256, 512, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(style_img_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define Loss Network \"\"\"\n",
    "\n",
    "StyleOutput = namedtuple(\"StyleOutput\", [\"relu1_1\", \"relu2_1\", \"relu3_1\", \"relu4_1\"])\n",
    "ContentOutput = namedtuple(\"ContentOutput\", [\"relu2_1\"])\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg = vgg\n",
    "        self.layer_name_mapping = {\n",
    "            '1': \"relu1_1\", '3': \"relu1_2\",\n",
    "            '6': \"relu2_1\", '8': \"relu2_2\",\n",
    "            '11': \"relu3_1\", '13': \"relu3_2\", '15': \"relu3_3\", '17': \"relu3_4\",\n",
    "            '20': \"relu4_1\", '22': \"relu4_2\", '24': \"relu4_3\", '26': \"relu4_4\",\n",
    "            '29': \"relu5_1\", '31': \"relu5_2\", '33': \"relu5_3\", '35': \"relu5_4\"\n",
    "        }\n",
    "\n",
    "    def forward(self, x, mode='style'):\n",
    "        #return of multiple outputs as dict\n",
    "        if mode == 'style':\n",
    "            layers = ['1', '6', '11', '20']\n",
    "        elif mode == 'content':\n",
    "            layers = ['6']\n",
    "        else:\n",
    "            print(\"Invalid mode. Select between 'style' and 'content'\")\n",
    "        output = {}\n",
    "        for name, module in self.vgg._modules.items():\n",
    "            x = module(x)\n",
    "            if name in layers:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        if mode == 'style':\n",
    "            return StyleOutput(**output)\n",
    "        else:\n",
    "            return ContentOutput(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load and extract features from VGG16 \"\"\"\n",
    "\n",
    "print(\"Loading VGG..\")\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "loss_network = LossNetwork(vgg).to(device).eval()\n",
    "del vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Before training, compute the features of every resolution of the style image \"\"\"\n",
    "\n",
    "print(\"Computing style features..\")\n",
    "with torch.no_grad(): \n",
    "    style_loss_features_256 = loss_network(Variable(style_img_256), 'style')\n",
    "    style_loss_features_512 = loss_network(Variable(style_img_512), 'style')\n",
    "    style_loss_features_1024 = loss_network(Variable(style_img_1024), 'style')\n",
    "gram_style_256 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_256]\n",
    "gram_style_512 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_512]\n",
    "gram_style_1024 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Init Net and loss \"\"\"\n",
    "\n",
    "style_subnet = StyleSubnet().to(device)\n",
    "enhance_subnet = EnhanceSubnet().to(device)\n",
    "refine_subnet = RefineSubnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare Training \"\"\"\n",
    "\n",
    "max_iterations = min(10000, len(train_dataset))\n",
    "\n",
    "# init loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "# init optimizer\n",
    "optimizer = torch.optim.Adam(list(style_subnet.parameters()) + \n",
    "                             list(enhance_subnet.parameters()) +\n",
    "                             list(refine_subnet.parameters()), lr=LR)\n",
    "\n",
    "def getLosses(generated_img, resized_input_img, content_weight, style_weight, mse_loss, gram_style):\n",
    "    \n",
    "    # Compute features\n",
    "    generated_style_features = loss_network(generated_img, 'style')\n",
    "    generated_content_features = loss_network(generated_img, 'content')\n",
    "    target_content_features = loss_network(resized_input_img, 'content')\n",
    "    \n",
    "    # Content loss\n",
    "    target_content_features = Variable(target_content_features[0].data, requires_grad=False)\n",
    "    content_loss = content_weight * mse_loss(generated_content_features[0], target_content_features)\n",
    "    \n",
    "    # Style loss\n",
    "    style_loss = 0.\n",
    "    for m in range(len(generated_style_features)):\n",
    "        gram_s = gram_style[m]\n",
    "        gram_y = gram_matrix(generated_style_features[m])\n",
    "        style_loss += style_weight * mse_loss(gram_y, gram_s.expand_as(gram_y))\n",
    "    \n",
    "    # Regularization loss\n",
    "    reg_loss = REG * (\n",
    "        torch.sum(torch.abs(generated_img[:, :, :, :-1] - generated_img[:, :, :, 1:])) + \n",
    "        torch.sum(torch.abs(generated_img[:, :, :-1, :] - generated_img[:, :, 1:, :])))\n",
    "    \n",
    "    return content_loss, style_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Perform Training \"\"\"\n",
    "\n",
    "style_subnet.train()\n",
    "enhance_subnet.train()\n",
    "refine_subnet.train()\n",
    "start = time.time()\n",
    "print(\"Start training on {}...\".format(device))\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    agg_content_loss, agg_style_loss, agg_reg_loss = 0., 0., 0.\n",
    "    log_counter = 0\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        # update learning rate every 2000 iterations\n",
    "        if i % 2000 == 0 and i != 0:\n",
    "            LR = LR * 0.8\n",
    "            optimizer = torch.optim.Adam(list(style_subnet.parameters()) + \n",
    "                                         list(enhance_subnet.parameters()) +\n",
    "                                         list(refine_subnet.parameters()), lr=LR)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_in = x.clone()\n",
    "        \n",
    "        \"\"\" Style Subnet \"\"\"\n",
    "        x_in = Variable(x_in).to(device)\n",
    "\n",
    "        # Generate image\n",
    "        generated_img_256, resized_input_img_256 = style_subnet(x_in)\n",
    "        resized_input_img_256 = Variable(resized_input_img_256.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        style_subnet_content_loss, style_subnet_style_loss, style_subnet_reg_loss = getLosses(\n",
    "            generated_img_256,\n",
    "            resized_input_img_256,\n",
    "            CONTENT_WEIGHTS[0],\n",
    "            STYLE_WEIGHTS[0],\n",
    "            mse_loss, gram_style_256)\n",
    "        \n",
    "        \"\"\" Enhance Subnet \"\"\"\n",
    "        x_in = Variable(generated_img_256)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_img_512, resized_input_img_512 = enhance_subnet(x_in)\n",
    "        resized_input_img_512 = Variable(resized_input_img_512.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        enhance_subnet_content_loss, enhance_subnet_style_loss, enhance_subnet_reg_loss = getLosses(\n",
    "            generated_img_512,\n",
    "            resized_input_img_512,\n",
    "            CONTENT_WEIGHTS[1],\n",
    "            STYLE_WEIGHTS[1],\n",
    "            mse_loss, gram_style_512)\n",
    "        \n",
    "        \"\"\" Refine Subnet \"\"\"\n",
    "        x_in = Variable(generated_img_512)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_img_1024, resized_input_img_1024 = refine_subnet(x_in)\n",
    "        resized_input_img_1024 = Variable(resized_input_img_1024.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        refine_subnet_content_loss, refine_subnet_style_loss, refine_subnet_reg_loss = getLosses(\n",
    "            generated_img_1024,\n",
    "            resized_input_img_1024,\n",
    "            CONTENT_WEIGHTS[2],\n",
    "            STYLE_WEIGHTS[2],\n",
    "            mse_loss, gram_style_1024)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = LAMBDAS[0] * (style_subnet_content_loss + style_subnet_style_loss + style_subnet_reg_loss) + \\\n",
    "                     LAMBDAS[1] * (enhance_subnet_content_loss + enhance_subnet_style_loss + enhance_subnet_reg_loss) + \\\n",
    "                     LAMBDAS[2] * (refine_subnet_content_loss + refine_subnet_style_loss + refine_subnet_reg_loss)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Aggregated loss\n",
    "        agg_content_loss += style_subnet_content_loss.data[0] + \\\n",
    "                            enhance_subnet_content_loss.data[0] + \\\n",
    "                            refine_subnet_content_loss.data[0]\n",
    "        agg_style_loss += style_subnet_style_loss.data[0] + \\\n",
    "                          enhance_subnet_style_loss.data[0] + \\\n",
    "                          refine_subnet_style_loss.data[0]\n",
    "        \n",
    "        agg_reg_loss += style_subnet_reg_loss.data[0] + \\\n",
    "                        enhance_subnet_reg_loss.data[0] + \\\n",
    "                        refine_subnet_reg_loss.data[0]\n",
    "                \n",
    "        # log training process\n",
    "        if (i + 1) % LOG_INTERVAL == 0:\n",
    "            log_counter += 1\n",
    "            hlp = log_counter * LOG_INTERVAL\n",
    "            time_per_pass = (time.time() - start) / hlp\n",
    "            estimated_time_left = (time_per_pass * (max_iterations - i))/3600\n",
    "            print(\"{} [{}/{}] time per pass: {:.2f}s  total time: {:.2f}s  estimated time left: {:.2f}h  content: {:.6f}  style: {:.6f}  reg: {:.6f}  total: {:.6f}\".format(\n",
    "                        time.ctime(), i+1, max_iterations,\n",
    "                        (time.time() - start) / hlp,\n",
    "                        time.time() - start,\n",
    "                        estimated_time_left,\n",
    "                        agg_content_loss / LOG_INTERVAL,\n",
    "                        agg_style_loss / LOG_INTERVAL,\n",
    "                        agg_reg_loss / LOG_INTERVAL,\n",
    "                        (agg_content_loss + agg_style_loss + agg_reg_loss) / LOG_INTERVAL))\n",
    "            agg_content_loss, agg_style_loss, agg_reg_loss = 0., 0., 0.\n",
    "            imshow(x, title=\"input image\")\n",
    "            imshow(generated_img_256, title=\"generated_img_256\")\n",
    "            imshow(generated_img_512, title=\"generated_img_512\")\n",
    "            imshow(generated_img_1024, title=\"generated_img_1024\")\n",
    "\n",
    "        # Stop training after max iterations\n",
    "        if (i + 1) == max_iterations: break\n",
    "\n",
    "\"\"\" Save model \"\"\"\n",
    "torch.save(style_subnet, 'models/style_subnet_picasso.pt')\n",
    "torch.save(enhance_subnet, 'models/enhance_subnet_picasso.pt')\n",
    "torch.save(refine_subnet, 'models/refine_subnet_picasso.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the git repository and just load the models, its more effective than training it for hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"picasso\"\n",
    "input_img = image_loader(\"maja.jpg\", size=256)\n",
    "imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "style_subnet = torch.load('models/style_subnet_picasso.pt', map_location='cpu').eval().to(device)\n",
    "enhance_subnet = torch.load('models/enhance_subnet_picasso.pt', map_location='cpu').eval().to(device)\n",
    "refine_subnet = torch.load('models/refine_subnet_picasso.pt', map_location='cpu').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start transforming on {}..\".format(device))\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    generated_img_256, resized_input_img_256 = style_subnet(input_img)\n",
    "    generated_img_512, resized_input_img_512 = enhance_subnet(generated_img_256)\n",
    "    generated_img_1024, resized_input_img_1024 = refine_subnet(generated_img_512)\n",
    "print(\"Image transformed. Time for pass: {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "imshow(generated_img_256)\n",
    "imshow(generated_img_512)\n",
    "imshow(generated_img_1024)\n",
    "save_image(generated_img_256, title=\"generated_images/multimodal_\" + MODEL + \"_256\")\n",
    "save_image(generated_img_512, title=\"generated_images/multimodal_\" + MODEL + \"_512\")\n",
    "save_image(generated_img_1024, title=\"generated_images/multimodal_\" + MODEL + \"_1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
